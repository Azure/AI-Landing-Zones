## Monitoring

| ID   | Specification |
|------|--------------|
| M-R1 | The AI Landing Zone should provide guidance and setup to monitor [AI models](https://learn.microsoft.com/azure/cloud-adoption-framework/scenarios/ai/platform/management#manage-ai-models), [AI resources](https://learn.microsoft.com/azure/cloud-adoption-framework/scenarios/ai/platform/management#manage-ai-operations), [AI data](https://learn.microsoft.com/azure/cloud-adoption-framework/scenarios/ai/platform/management#manage-ai-data) to ensure that it remains aligned with customers KPIs |
| M-R2 | **AMBA**: The AI Landing Zone should provide guidance and implementation of recommended alerts leveraging AMBA.<br><br>Best Practice:<br><br>Enable recommended alert rules to receive notifications of deviations that indicate a decline in workload health. For examples, see [Azure AI Search](https://learn.microsoft.com/azure/search/monitor-azure-cognitive-search#azure-ai-search-alert-rules), [Azure Machine Learning](https://learn.microsoft.com/azure/machine-learning/monitor-azure-machine-learning), and guidance on individual Azure AI services. |
| M-R3 | **Monitor generative AI:** Provide guidance on monitoring performance of generative AI applications.<br><br>Best Practice:<br><br>Use Azure AI Foundry evaluations (quality & safety metrics) and tracing to collect per‑request spans (prompt → model call → tool). Consolidate token usage, latency, and evaluation scores. Use responsible AI tooling to extend safety and bias coverage. Replace deprecated prompt flow monitoring guidance with standardized agent/service telemetry. |
| M-R4 | **Monitor nongenerative AI:** The AI Landing Zone should provide guidance on how to monitor the performance of a nongenerative AI application that leverages the AI Landing Zone<br><br>Best Practice:<br><br>For nongenerative AI workloads, monitor data processing stages and model performance metrics to ensure predictions remain accurate and reliable. Enable [model monitoring](https://learn.microsoft.com/azure/machine-learning/concept-model-monitoring) in Azure Machine Learning. For Azure AI services, enable monitoring for each AI service you use. |
| M-R5 | **Monitor generative AI performance:** Provide guidance on key performance indicators.<br><br>Best Practice:<br><br>Monitor end‑to‑end latency (frontend → gateway → model), vector recall / relevance scores, groundedness, and tool invocation success. Enable tracing to collect per span tokens, latency, and errors. Correlate with APIM token limit / circuit breaker metrics for holistic view. |
| M-R6 | **Monitor non-generative AI performance:** The AI Landing Zone should provide guidance on how to monitor the performance of a non-generative AI application that leverages the AI Landing Zone<br><br>Best Practice:<br><br>Capture [performance metrics](https://learn.microsoft.com/azure/machine-learning/how-to-monitor-model-performance#set-up-model-performance-monitoring) of models deployed in Azure Machine Learning. For Azure AI services, enable [diagnostic logging](https://learn.microsoft.com/azure/ai-services/diagnostic-logging) for each Azure AI service. |
| M-R7 | **Monitor platform resources:** Capture logs & metrics of all deployed resources to a central Log Analytics workspace.<br><br>Best Practice:<br><br>Configure diagnostic settings for AI Foundry, Azure Machine Learning, Azure OpenAI, API Management, Container Apps, Search, Cosmos DB, Storage, Key Vault. Include categories: Audit, Request, GatewayLogs, Operation, Billing (where available). Use Azure Monitor Baseline Alerts for baseline + custom alerts (token anomaly, cache hit ratio drop, circuit breaker open). |
| M-R8 | **Monitor model & data drift:** Provide guidance on monitoring drift signals.<br><br>Best Practice:<br><br>Track relevance / groundedness score trends, embedding distribution changes, prompt outcome variance, and domain KPIs. Use evaluations in Azure AI Foundry and AML model monitoring metrics. Trigger retraining review when drift metric crosses threshold (e.g., >5% degradation over 7 days). |
| M-R9 | **Network Troubleshooting:** Provide guidance on leveraging Azure Monitor insights and Network Watcher.<br><br>Best Practice:<br><br>Use Connection Monitor, NSG flow logs, and APIM latency diagnostics to isolate network vs application bottlenecks. |
| M-R10 | **Gateway (APIM) Policy Telemetry:** Observe token limit triggers, semantic cache hit ratio, circuit breaker trips, fallback routing events.<br><br>Best Practice:<br><br>Attach APIM policies (token-limit, emit-token-metric, semantic-cache, retry/circuit-breaker). Export metrics and consolidate with application traces using `operationId`. Alert on sustained circuit breaker open state (>2 min) or low cache hit (<20% after warm-up). |
| M-R11 | **Cost & Token Observability:** Provide standardized token and cost metrics (tokensPrompt, tokensCompletion, tokensCached, estimatedCost).<br><br>Best Practice:<br><br>Aggregate APIM emitted token metrics + application side estimation; enrich with model price table (parameter file) to derive cost per request & conversation. |

### Additional Implementation Guidance
1. Correlation: Propagate W3C `traceparent` across APIM → application → model call; include `conversationId` as custom dimension.
2. Token Cost: Maintain pricing parameter file and enrich metrics with calculated `estimatedCost`.
3. Evaluation / Batch: Log batch job submission, completion, token totals, and per‑job unit cost separately from interactive traffic.
4. Dashboards: Provide panels: Reliability (errors/latency), Cost (tokens, cache hit %), Governance (disallowed model attempts), Security (injection flagged events).
5. Retention: Detailed per request logs 30 days; aggregated metrics 400 days (cost/performance trends).

